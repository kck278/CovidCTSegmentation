

Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

###############################################
I am running the following nnUNet: 2d
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 5, 'num_pool_per_axis': [7, 7], 'patch_size': array([512, 512]), 'median_patient_size_in_voxels': array([  1, 512, 512]), 'current_spacing': array([1., 1., 1.]), 'original_spacing': array([1., 1., 1.]), 'pool_op_kernel_sizes': [[2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'conv_kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'do_dummy_2D_data_aug': False}

I am using stage 0 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /home/hd/hd_hd/hd_ei260/CovidCTSegmentation/nnUNet/nnUNet_preprocessed/Task501_Covid/nnUNetData_plans_v2.1_2D
###############################################
loading dataset
loading all case properties
unpacking dataset
done
2021-09-20 19:50:41.984794: lr: 0.01
using pin_memory on device 0
/home/hd/hd_hd/hd_ei260/miniconda3/envs/covid_seg/lib/python3.9/site-packages/nnunet/training/network_training/nnUNetTrainerV2.py:254: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  torch.nn.utils.clip_grad_norm_(self.network.parameters(), 12)
slurmstepd: error: *** JOB 19856020 ON uc2n485 CANCELLED AT 2021-09-20T20:49:10 DUE TO TIME LIMIT ***

============================= JOB FEEDBACK =============================

NodeName=uc2n485
Job ID: 19856020
Cluster: uc2
User/Group: hd_ei260/hd_hd
State: TIMEOUT (exit code 0)
Nodes: 1
Cores per node: 20
CPU Utilized: 04:19:02
CPU Efficiency: 21.53% of 20:03:20 core-walltime
Job Wall-clock time: 01:00:10
Memory Utilized: 41.00 GB
Memory Efficiency: 410.01% of 10.00 GB
